<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Junyan  Su | Paper Summary: Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üî•</text></svg>">

<link rel="stylesheet" href="/al-folio/assets/css/main.css">

<link rel="canonical" href="/al-folio/blog/2021/aSGD/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/al-folio/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/al-folio/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/al-folio/">
       <span class="font-weight-bold">Junyan</span>   Su
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/al-folio/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/al-folio/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/al-folio/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/al-folio/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/al-folio/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Paper Summary: Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization</h1>
    <p class="post-meta">February 10, 2021</p>
  </header>

  <article class="post-content">
    <p>In this blog, I will give a summary of the paper <a href="https://icml.cc/2012/papers/261.pdf">‚ÄúMaking Gradient Descent Optimal for Strongly Convex Stochastic Optimization‚Äù</a> and some of my understandings. A full version with proofs can be found at <a href="https://arxiv.org/pdf/1109.5647.pdf">arxiv</a>. For stardard stochastic gradient descent (SGD) method with strongly convex problem, the convergence rate is known to be \(O(log(T)/T)\), while there are other algorithms achieves \(O(1/T)\) convergence rate. Is \(O(log(T)/T)\) a tight bound for SGD?
In this paper, the authors give an example of strongly convex and non-smooth problem where the stardard stochastic gradient descent (SGD) has a \(\Omega(log(T)/T)\). This justify the claim that \(O(log(T)/T)\) is a tight bound for stardard SGD. Moreover, a simple modification of SGD is shown to be sufficient to recover the \(O(1/T)\) rate.</p>

<h3 id="1-stochastic-gradient-descent">1. Stochastic Gradient Descent</h3>
<p>Stochastic Gradient Descent (SGD) is a well-known and popular method to solve a convex stochastic optimization problem. The goal is to minimize the objective \(F\) over some convex domain \(W\), without the knowledge of \(F\). The only information we can obtain is a stochastic gradient oracle. For each input \(w\in W\), the oracle will output a vector \(\hat{g}\) whose expectation \(E(\hat{g}) = g \in \partial F(w)\) is a subgradient of \(F\) at \(w\).</p>

<p>In SGD method, we first arbitrarily select a initial point \(w\) inside the convex domain \(W\). Then at each iteration step \(t=1,\dots,T\), we call the stochastic gradient oracle to obtain the \(\hat{g}_t\), then update the optimization variable by</p>

\[w_{t+1} = \Pi_{W}(w_t-\eta_t \hat{g}_t)\]

<p>Here, the step size \(\eta_t\) is designed by users. \(\Pi_{W}\) is the projection operator on the convex domain \(W\). After getting the sequence of points \(w_1,\dots,w_T\), stardard SGD returns the average point</p>

\[\bar{w}_T = \frac{1}{T}(w_1+\dots+w_T)\]

<p>For \(\lambda\)-strongly convex function \(F\), the authors consider the general step sizes \(\eta_t = {c}/({\lambda t})\) and it can be shown that the step size of \(\Theta(1/t)\) is necessary to obtain the optimal convergence rate.
For smooth problem, the last point of SGD converges with \(O(1/T)\). Mathemcatically, suppose \(F\) is \(\lambda\)-stronly convex and \(\mu\)-smooth with repect to the optimal point \(w^*\), and suppose \(E[|| \hat{g}_t||^2] \leq G^2\). Then with step size \(\eta_t = c/(\lambda t)\) and \(c&gt;1/2\), we have:</p>

\[E[F(W_T)-F(w^*)] \leq \frac{1}{2}max\{ 4,\; \frac{c}{2-1/c} \} \frac{\mu G^2}{\lambda^2 T}\]

<p>This inequality claims that the expected difference between the objective at last point \(F(w_T)\) and optimal value \(F(w^*)\) is at most order of \((1/T)\). For the average point \(\bar{w}_T\), it also enjoys an \(O(1/T)\) rate.</p>

<h3 id="2-non-smooth-problems">2. Non-Smooth Problems</h3>

<p>So far, SGD works well with stronly convex and smooth objective functions. But what about non-smooth functions? In the literature, SGD with averaging is known to have a \(O(log(T)/T)\). 
Besides, there are other optimal algorithms that achieve \(O(1/T)\). One natural question to ask is what makes this gap. Is it because of the property of SGD or the loose analysis? In the paper, the authors give a simple example, on which SGD with averaging has a \(\Omega(log(T)/T)\). This actually shows that the convergence rate \(O(log(T)/T)\) is tight. The example is given as follows. Let \(F\) be the function:</p>

\[F(w) = \frac{1}{2} ||w||^2 + w_1\]

<p>where the domain \(W=[0,1]^d\). Upon enquries, the gradient oracle produces the estimate \(\hat{g}_t = w_t + (Z_t,0,\dots,0)\). The random variable \(Z_t\) is uniformly distributed over \([-1,3]\). As claimed in theorem 3, there exist a \(T_0\) such that the following inequality holds for any \(T \geq T_0 + 1\).</p>

\[E[F(\bar{w}_T)-F(w^*)] \geq \frac{c}{16T} \sum_{t=T_0}^{T-1} \frac{1}{t}\]

<p>Therefore, the lower bound of SGD with averaging is \(\Omega(log(T)/T)\). The intuition behind this example is that the non-smooth function \(F\) is designed such that the iterates \(w_t\) appoach the optimum from just one direction. This example shows that the convergence rate \(O(log(T)/T)\) is indeeed tight for SGD with averaging, which is the property of this method.</p>

<h3 id="3-sgd-with-alpha-suffix-averaging">3. SGD with \(\alpha\)-Suffix Averaging</h3>

<p>In the literature, there are some algorithms that achieve the optimal \(O(1/T)\) rate for any \(F\). But those algorithms are complicated and different from the idea of SGD. This paper gives a rather simple modification to standard SGD. Instead of averaging all the iterates \(w_t\), the modified version focus on last \(\alpha\)-proportion of \(T\) iterations. In particular, given a constant \(\alpha \in (0,1)\), the returned value is given by:</p>

\[\bar{w}^{\alpha}_T = \frac{w_{(1-\alpha)T+1}+\dots+w_T}{\alpha T}\]

<p>where we assume \(\alpha T\) and \((1-\alpha)T\) are integers. This method is called \(\alpha\)-suffix averaging. As claimed in theorem 5, the expected difference is bounded by the following inequality:</p>

\[E[F(\bar{w}_T^{\alpha})-F(w^*)] \leq c_0 \cdot \frac{log(\frac{1}{1-\alpha})}{\alpha} \cdot \frac{G^2}{\lambda T}\]

<p>where \(c_0\) is a constant factor that only depends on \(c\). And for any \(\alpha \in (0,1)\), the bound is \(O(G^2/\lambda T)\) which approaches the optimal guarantees up to constant factors. The table below summaries the convergence rate for the different methods and different assumptions discussed in the paper. In the table, SGD-A is standard SGD with averaging, SGD-\(\alpha\) is SGD with \(\alpha\)-suffix averaging as discussed in this section, EPOCH-GD is the optimal algorithm proposed in [1].</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>strongly convex</th>
      <th>non-smooth</th>
      <th>rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SGD-A</td>
      <td>‚òë</td>
      <td>‚òí</td>
      <td>\(O(1/T)\)</td>
    </tr>
    <tr>
      <td>¬†</td>
      <td>‚òë</td>
      <td>‚òë</td>
      <td>\(O(log(T)/T)\)</td>
    </tr>
    <tr>
      <td>SGD-\(\alpha\)</td>
      <td>‚òë</td>
      <td>‚òë</td>
      <td>\(O(1/T)\)</td>
    </tr>
    <tr>
      <td>EPOCH-GD</td>
      <td>‚òë</td>
      <td>‚òë</td>
      <td>\(O(1/T)\)</td>
    </tr>
  </tbody>
</table>

<p>Besides, the figure below (figure 2 in the original paper) shows the experiment results of different algorithms in example constructed in the previous section. The additional SGD-L plot is for SGD return with last iterate. We can observe from the figure that the SGD-A (blue line) indeed suffers from a \(O(log(T)/T)\) tight bound in this example.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/al-folio/assets/img/2021-02-10-aSGD-result.png" />
    </div>
</div>

<h3 id="4-conclusion">4. Conclusion</h3>

<p>This paper shows the tight bound of standard SGD by an example. The SGD with \(\alpha\)-suffix averaging is proposed to close the gap between standard SGD and the optimal algorithm. The numerical results valid the theoretical analysis. I really enjoy reading this paper since everything is clearly addressed.</p>

<h3 id="references">References</h3>

<p>[1] Hazan, E. and Kale, S. Beyond the regret minimization barrier: An optimal algorithm for stochastic strongly-convex optimization. In COLT, 2011.</p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Junyan  Su.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/al-folio/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/al-folio/assets/js/common.js"></script>


</html>
